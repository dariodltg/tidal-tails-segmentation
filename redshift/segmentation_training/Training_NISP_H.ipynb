{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26c8ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "from comet_ml import Experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torchmetrics import Dice\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baaa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparametros = {'nombre_notebook': 'Training_NISP_H_1000imgs.ipynb',\n",
    "                    'nombre_experimento' : 'Training_NISP_H_1000imgs',\n",
    "                    'nombre_mejor_modelo_a_guardar' : 'NISP_H_1000imgs',\n",
    "                    'device' : torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                    'ruta_galaxias' : 'galaxies_train_NISP_H/galaxy_and_stream_convolved*',\n",
    "                    'ruta_mascaras' : 'masks_train_NISP_H/mask_',\n",
    "                    'ruta_galaxias_test' : 'galaxies_test_NISP_H/galaxy_and_stream_convolved*.fits',\n",
    "                    'ruta_mascaras_test' : 'masks_test_NISP_H/mask_',\n",
    "                    'ancho_imagen_original' : 200,\n",
    "                    'alto_imagen_original' : 200,\n",
    "                    'ancho_imagen_deseado' : 224,\n",
    "                    'alto_imagen_deseado' : 224,\n",
    "                    'epocas' : 500,\n",
    "                    'lr' : 1e-3,\n",
    "                    'regularizacion_ridge' : 1e-5,\n",
    "                    'torch seed model weights' : 10,\n",
    "                    'batch_size' : 4,\n",
    "                    'loss' : torch.nn.CrossEntropyLoss()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46b4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"dhgO2PK521nSSpRLBYPemvvs7\",\n",
    "    project_name=\"general\",\n",
    "    workspace=\"dario-torre\",\n",
    "    \n",
    ")\n",
    "experiment.set_name(hiperparametros['nombre_experimento'])\n",
    "experiment.log_parameters(hiperparametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = hiperparametros['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d56ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_galaxy_number(galaxy_name:str):\n",
    "    return os.path.basename(galaxy_name).split('_')[4]\n",
    "\n",
    "def get_galaxy_magnitude(galaxy_name:str):\n",
    "    return os.path.basename(galaxy_name).split('_')[5]\n",
    "\n",
    "def normalize_01(inp: np.ndarray):\n",
    "    \"\"\"Squash image input to the value range [0, 1] (no clipping)\"\"\"\n",
    "    inp_out = (inp - np.min(inp)) / np.ptp(inp)\n",
    "    return inp_out\n",
    "\n",
    "def imagen_logaritmica(img: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Escalamos de forma logarítmica la imagen para aumentar el contraste\n",
    "    \"\"\"\n",
    "    img_log = np.log(img, where=(img!=0))\n",
    "    valor_minimo = np.min(img_log)\n",
    "    np.putmask(img_log, img!=0, img_log+abs(valor_minimo))\n",
    "    return img_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset básico sin ningún tipo de augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, galaxias_con_colas_de_marea, transform=None):\n",
    "        \"\"\"\n",
    "        Constructor del dataset\n",
    "        @param galaxias_con_colas_de_marea: Lista de rutas a los ficheros que contienen los datos de las galaxias\n",
    "        \"\"\"\n",
    "        self.galaxias_con_colas_de_marea = galaxias_con_colas_de_marea\n",
    "        self.transform=transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Abrimos la imagen de la galaxia con cola de marea\n",
    "        galaxy_fits = fits.open(self.galaxias_con_colas_de_marea[index])\n",
    "        x = galaxy_fits[0].data.astype(np.float32)\n",
    "        #Abrimos la mascara correspondiente a la imagen\n",
    "        numero_galaxia = get_galaxy_number(self.galaxias_con_colas_de_marea[index])\n",
    "        magnitud_galaxia = get_galaxy_magnitude(self.galaxias_con_colas_de_marea[index])\n",
    "        y = np.array(cv2.imread(hiperparametros['ruta_mascaras']+str(numero_galaxia)+\"_\"+str(magnitud_galaxia)+\".png\",0)).astype(np.float32)\n",
    "        x = normalize_01(x)\n",
    "        x = imagen_logaritmica(x)\n",
    "        x = normalize_01(x)\n",
    "        # Assuming 'input_item' is your input image\n",
    "        x = cv2.resize(x, (hiperparametros['ancho_imagen_deseado'], hiperparametros['alto_imagen_deseado']))\n",
    "        y = cv2.resize(y, (hiperparametros['ancho_imagen_deseado'], hiperparametros['alto_imagen_deseado']))\n",
    "        if(self.transform is not None):\n",
    "            augmented = self.transform(image=x, mask=y)\n",
    "            x_tensor= augmented[\"image\"]\n",
    "            y_tensor= augmented[\"mask\"].long()\n",
    "        else:\n",
    "            x_tensor = torch.from_numpy(x).float()\n",
    "            x_tensor=torch.unsqueeze(x_tensor, dim=0)\n",
    "            y_tensor = torch.from_numpy(y).long()\n",
    "        \n",
    "        #Hacemos reshape de los tensores\n",
    "        y_tensor=torch.unsqueeze(y_tensor, dim=0)\n",
    "        return x_tensor, y_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Devuelve la longitud del dataset\n",
    "        \"\"\"\n",
    "        return len(self.galaxias_con_colas_de_marea)\n",
    "    \n",
    "class MyDatasetTest(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset básico sin ningún tipo de augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, galaxias_con_colas_de_marea, transform=None):\n",
    "        \"\"\"\n",
    "        Constructor del dataset\n",
    "        @param galaxias_con_colas_de_marea: Lista de rutas a los ficheros que contienen los datos de las galaxias\n",
    "        \"\"\"\n",
    "        self.galaxias_con_colas_de_marea = galaxias_con_colas_de_marea\n",
    "        self.transform=transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Abrimos la imagen de la galaxia con cola de marea\n",
    "        galaxy_fits = fits.open(self.galaxias_con_colas_de_marea[index])\n",
    "        x = galaxy_fits[0].data.astype(np.float32)\n",
    "        #Abrimos la mascara correspondiente a la imagen\n",
    "        numero_galaxia = get_galaxy_number(self.galaxias_con_colas_de_marea[index])\n",
    "        magnitud_galaxia = get_galaxy_magnitude(self.galaxias_con_colas_de_marea[index])\n",
    "        y = np.array(cv2.imread(hiperparametros['ruta_mascaras_test']+str(numero_galaxia)+\"_\"+str(magnitud_galaxia)+\".png\",0)).astype(np.float32)\n",
    "        x = normalize_01(x)\n",
    "        x = imagen_logaritmica(x)\n",
    "        x = normalize_01(x)\n",
    "        # Assuming 'input_item' is your input image\n",
    "        x = cv2.resize(x, (hiperparametros['ancho_imagen_deseado'], hiperparametros['alto_imagen_deseado']))\n",
    "        y = cv2.resize(y, (hiperparametros['ancho_imagen_deseado'], hiperparametros['alto_imagen_deseado']))\n",
    "        if(self.transform is not None):\n",
    "            augmented = self.transform(image=x, mask=y)\n",
    "            x_tensor= augmented[\"image\"]\n",
    "            y_tensor= augmented[\"mask\"].long()\n",
    "        else:\n",
    "            x_tensor = torch.from_numpy(x).float()\n",
    "            x_tensor=torch.unsqueeze(x_tensor, dim=0)\n",
    "            y_tensor = torch.from_numpy(y).long()\n",
    "        \n",
    "        #Hacemos reshape de los tensores\n",
    "        y_tensor=torch.unsqueeze(y_tensor, dim=0)\n",
    "        return x_tensor, y_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Devuelve la longitud del dataset\n",
    "        \"\"\"\n",
    "        return len(self.galaxias_con_colas_de_marea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = A.Compose([\n",
    "    A.Flip(),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb177b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imagenes_train = glob.glob(hiperparametros['ruta_galaxias'])\n",
    "imagenes_valid = glob.glob(hiperparametros['ruta_galaxias_test'])\n",
    "print(\"Number of train images: \" + str(len(imagenes_train)))\n",
    "print(\"Number of valid images: \" + str(len(imagenes_valid)))\n",
    "dataset_augmentations = MyDataset(imagenes_train)\n",
    "train_dataset= ConcatDataset([dataset_augmentations])\n",
    "valid_dataset= MyDatasetTest(imagenes_valid)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=hiperparametros['batch_size'], shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=hiperparametros['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc2c7e",
   "metadata": {},
   "source": [
    "## Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c246c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = smp.Unet(\n",
    "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc070f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(hiperparametros['torch seed model weights'])\n",
    "unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37708372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imprimimos la arquitectura de la red\n",
    "#summary(unet, (1, 608, 608))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280e05c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Comprobamos que hemos construido la arquitectura de la red correctamente cargando una imagen del dataset\n",
    "input_item, label_item = train_dataset.__getitem__(0)\n",
    "input_item = input_item.reshape((1,1,hiperparametros['ancho_imagen_deseado'],hiperparametros['alto_imagen_deseado']))\n",
    "label_item = label_item.reshape((1,1,hiperparametros['ancho_imagen_deseado'],hiperparametros['alto_imagen_deseado']))\n",
    "output_item = unet(input_item.to(device)).cpu().detach().squeeze()\n",
    "print(output_item.shape)\n",
    "out_softmax = torch.softmax(output_item, dim=0)\n",
    "mascara_predicha = torch.argmax(out_softmax, dim=0).numpy()\n",
    "fig, (axs0, axs1) = plt.subplots(1,2, figsize = (15,15))\n",
    "axs0.imshow(input_item.squeeze(), interpolation='none', origin=\"lower\")\n",
    "axs1.imshow(label_item.squeeze(), interpolation='none', origin=\"lower\")\n",
    "#plt.imshow(mascara_predicha, interpolation='none', origin=\"lower\")\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bd825",
   "metadata": {},
   "source": [
    "## Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42030d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m dice_medio_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m pasos_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     19\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataset.py:335\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m     x_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     33\u001b[0m     x_tensor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39munsqueeze(x_tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     y_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#Hacemos reshape de los tensores\u001b[39;00m\n\u001b[1;32m     37\u001b[0m y_tensor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39munsqueeze(y_tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterio_loss = hiperparametros['loss']\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=hiperparametros['lr'], weight_decay = hiperparametros['regularizacion_ridge'])\n",
    "mejor_modelo = None\n",
    "mejor_loss = 100000000\n",
    "#Inicializamos la métrica de Dice\n",
    "dice = Dice(num_classes=2, average='macro', ignore_index=0)\n",
    "dice.to(device)\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "train_dice_list = []\n",
    "valid_dice_list = []\n",
    "for epoch in range(hiperparametros['epocas']):\n",
    "    experiment.set_epoch(epoch)\n",
    "    #Parte de train\n",
    "    loss_medio_train = 0\n",
    "    dice_medio_train=0\n",
    "    pasos_train = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = unet(inputs)\n",
    "        targets = torch.squeeze(labels, dim=1).type(torch.LongTensor).to(device)\n",
    "        loss = criterio_loss(outputs, targets)\n",
    "        valor_dice = dice(outputs, targets)\n",
    "        loss_medio_train += loss.item()\n",
    "        dice_medio_train += valor_dice.item()\n",
    "        pasos_train += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    experiment.log_metric('Loss train', loss_medio_train/pasos_train)\n",
    "    experiment.log_metric('Dice train', dice_medio_train/pasos_train)\n",
    "    unet.eval()\n",
    "    \n",
    "    #Parte de validacion\n",
    "    loss_medio_valid = 0\n",
    "    dice_medio_valid = 0\n",
    "    pasos_valid = 0\n",
    "    \n",
    "    for i, data in enumerate(valid_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = unet(inputs)\n",
    "        targets = torch.squeeze(labels, dim=1).type(torch.LongTensor).to(device)        \n",
    "        loss = criterio_loss(outputs, targets)\n",
    "        valor_dice = dice(outputs, targets)\n",
    "        loss_medio_valid += loss.item()\n",
    "        dice_medio_valid += valor_dice.item()\n",
    "        pasos_valid += 1\n",
    "    experiment.log_metric('Loss valid', loss_medio_valid/pasos_valid)\n",
    "    experiment.log_metric('Dice valid', dice_medio_valid/pasos_valid)\n",
    "    #Resultados del epoch\n",
    "    print(\"Época: \"+ str(epoch) +\": Loss_train:\"+ str(loss_medio_train/pasos_train)+\" Loss_valid:\"+str(loss_medio_valid/pasos_valid)+ \"\\n Dice_train: \"+ str(dice_medio_train/pasos_train)+ \" Dice_valid: \"+str(dice_medio_valid/pasos_valid))\n",
    "    train_loss_list.append(loss_medio_train/pasos_train)\n",
    "    valid_loss_list.append(loss_medio_valid/pasos_valid)\n",
    "    if (loss_medio_valid/pasos_valid) < mejor_loss:\n",
    "        mejor_modelo = copy.deepcopy(unet)\n",
    "        mejor_loss = loss_medio_valid/pasos_valid\n",
    "    unet.train()\n",
    "print('\\n\\nFinished Training')\n",
    "print('Mejor loss sobre validación alcanzado:', mejor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mejor_modelo.state_dict(), hiperparametros['nombre_mejor_modelo_a_guardar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4501255",
   "metadata": {},
   "source": [
    "## Comprobar resultados con el conjunto de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eba29e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ancho = hiperparametros['ancho_imagen_deseado']\n",
    "alto =hiperparametros['alto_imagen_deseado']\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "mejor_modelo.eval()\n",
    "for i in range(int(len(train_dataset)*0.01)):\n",
    "    item_dataset = train_dataset.__getitem__(i)\n",
    "    item_x = item_dataset[0].reshape((1,1,ancho,alto)).cpu().detach().squeeze().numpy()\n",
    "    item_label = item_dataset[1].reshape((1,1,ancho,alto)).cpu().detach().squeeze().numpy()\n",
    "    #Obtenemos predicción de la red\n",
    "    prediccion_item_tensor = mejor_modelo(item_dataset[0].reshape((1,1,ancho,alto)).to(device)).cpu().detach().squeeze()\n",
    "    out_softmax = torch.softmax(prediccion_item_tensor, dim=0) #Aplicamos una softmax al final (teoricamente no afecta)\n",
    "    mascara_predicha = torch.argmax(out_softmax, dim=0).numpy()\n",
    "    #Mostramos imagen, mascara y mascara predicha\n",
    "    fig, (axs0, axs1, axs2) = plt.subplots(1,3, figsize = (15,15))    \n",
    "    axs0.imshow(item_x, interpolation='none', origin=\"lower\")\n",
    "    axs1.imshow(item_label, interpolation='none', origin=\"lower\")\n",
    "    axs2.imshow(mascara_predicha, interpolation='none', origin=\"lower\")\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8d5ad",
   "metadata": {},
   "source": [
    "## Comprobar resultados con el conjunto de validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b0aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ancho = hiperparametros['ancho_imagen_deseado']\n",
    "alto =hiperparametros['alto_imagen_deseado']\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "mejor_modelo.eval()\n",
    "for i in range(int(len(valid_dataset)*0.2)):\n",
    "    item_dataset = valid_dataset.__getitem__(i)\n",
    "    item_x = item_dataset[0].reshape((1,1,ancho,alto)).cpu().detach().squeeze().numpy()\n",
    "    item_label = item_dataset[1].reshape((1,1,ancho,alto)).cpu().detach().squeeze().numpy()\n",
    "    #Obtenemos predicción de la red\n",
    "    prediccion_item_tensor = mejor_modelo(item_dataset[0].reshape((1,1,ancho,alto)).to(device)).cpu().detach().squeeze()\n",
    "    mascara_predicha = torch.argmax(prediccion_item_tensor, dim=0).numpy()\n",
    "    #Mostramos imagen, mascara y mascara predicha\n",
    "    fig, (axs0, axs1, axs2) = plt.subplots(1,3, figsize = (15,15))\n",
    "    axs0.imshow(item_x, interpolation='none', origin=\"lower\")\n",
    "    axs1.imshow(item_label, interpolation='none', origin=\"lower\")\n",
    "    axs2.imshow(mascara_predicha, interpolation='none', origin=\"lower\")\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99f000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095f70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
